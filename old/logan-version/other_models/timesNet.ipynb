{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41b455ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Added /Users/loganyamamoto/Desktop/class/CSCI/566/project/StockPredictor/Time-Series-Library to Python path\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add Time-Series-Library to Python path so we can import layers\n",
    "# The notebook is in other_models/, and Time-Series-Library is at the project root\n",
    "cwd = os.getcwd()\n",
    "if 'other_models' in cwd:\n",
    "    # If we're in other_models directory, go up one level\n",
    "    project_root = os.path.dirname(cwd)\n",
    "else:\n",
    "    # Otherwise assume we're at project root\n",
    "    project_root = cwd\n",
    "\n",
    "timeseries_lib_path = os.path.join(project_root, 'Time-Series-Library')\n",
    "\n",
    "if os.path.exists(timeseries_lib_path):\n",
    "    if timeseries_lib_path not in sys.path:\n",
    "        sys.path.insert(0, timeseries_lib_path)\n",
    "    print(f\"✅ Added {timeseries_lib_path} to Python path\")\n",
    "else:\n",
    "    print(f\"⚠️  Warning: Could not find Time-Series-Library at {timeseries_lib_path}\")\n",
    "    print(f\"   Current directory: {cwd}\")\n",
    "    print(f\"   Looking for: {timeseries_lib_path}\")\n",
    "\n",
    "import torch    \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.fft\n",
    "from layers.Embed import DataEmbedding\n",
    "from layers.Conv_Blocks import Inception_Block_V1   \n",
    "# convolution block used for convoluting the 2D time data, changeable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5897a2a9",
   "metadata": {},
   "source": [
    "# TimesNet Configuration Parameters Guide\n",
    "\n",
    "This guide explains all configuration parameters needed for TimesNet model initialization.\n",
    "\n",
    "## Required Parameters for Classification Task\n",
    "\n",
    "Based on your data shape `(1445936, 31, 13)`, here are the key parameters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd67c03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TimesNet Configuration Parameters Explained\n",
    "\n",
    "class TimesNetConfig:\n",
    "    \"\"\"\n",
    "    Complete guide to TimesNet configuration parameters.\n",
    "    Based on your data: (batch_size=1445936, seq_len=31, features=13)\n",
    "    \"\"\"\n",
    "    \n",
    "    # ============================================\n",
    "    # TASK CONFIGURATION\n",
    "    # ============================================\n",
    "    task_name = 'classification'  # Options: 'long_term_forecast', 'short_term_forecast', \n",
    "                                   #          'imputation', 'classification', 'anomaly_detection'\n",
    "    \n",
    "    # ============================================\n",
    "    # DATA DIMENSIONS (CRITICAL FOR YOUR DATA)\n",
    "    # ============================================\n",
    "    seq_len = 31          # Input sequence length (timelength dimension)\n",
    "                          # YOUR DATA: 31 days lookback window\n",
    "    \n",
    "    enc_in = 13           # Encoder input size = number of features per timestep\n",
    "                          # YOUR DATA: 13 features per day\n",
    "    \n",
    "    c_out = 13            # Output size (for forecast/imputation/anomaly)\n",
    "                          # For classification, this is ignored\n",
    "    \n",
    "    num_class = 3         # Number of classification classes\n",
    "                          # YOUR TASK: Binary classification (up/down)\n",
    "    \n",
    "    # ============================================\n",
    "    # MODEL ARCHITECTURE\n",
    "    # ============================================\n",
    "    d_model = 512         # Dimension of model embeddings (hidden dimension)\n",
    "                          # Typical values: 64, 128, 256, 512, 1024\n",
    "                          # Larger = more capacity but slower\n",
    "    \n",
    "    d_ff = 2048           # Dimension of feed-forward network (FFN)\n",
    "                          # Usually 4x d_model (512 * 4 = 2048)\n",
    "    \n",
    "    e_layers = 2          # Number of encoder layers (TimesBlock layers)\n",
    "                          # More layers = deeper model, but risk of overfitting\n",
    "                          # Typical: 2-4 layers\n",
    "    \n",
    "    # ============================================\n",
    "    # TIMESBLOCK SPECIFIC PARAMETERS\n",
    "    # ============================================\n",
    "    top_k = 5             # Number of top frequencies to consider in FFT\n",
    "                          # TimesNet finds top_k periodic patterns\n",
    "                          # Typical: 3-5\n",
    "    \n",
    "    num_kernels = 6       # Number of kernels in Inception_Block_V1\n",
    "                          # More kernels = more diverse convolution patterns\n",
    "                          # Typical: 6\n",
    "    \n",
    "    # ============================================\n",
    "    # FORECASTING TASK PARAMETERS (not used for classification)\n",
    "    # ============================================\n",
    "    pred_len = None         # Prediction sequence length (for forecast tasks)\n",
    "    label_len = None        # Start token length (for forecast tasks)\n",
    "    \n",
    "    # ============================================\n",
    "    # EMBEDDING CONFIGURATION\n",
    "    # ============================================\n",
    "    embed = 'timeF'       # Time features encoding type\n",
    "                          # Options: 'timeF', 'fixed', 'learned'\n",
    "                          # - 'timeF': Time feature embedding (recommended)\n",
    "                          # - 'fixed': Fixed sinusoidal positional embedding\n",
    "                          # - 'learned': Learnable positional embedding\n",
    "    \n",
    "    freq = 'd'            # Frequency for time features encoding\n",
    "                          # Options: 's' (secondly), 't' (minutely), 'h' (hourly),\n",
    "                          #          'd' (daily), 'b' (business days), 'w' (weekly),\n",
    "                          #          'm' (monthly)\n",
    "                          # YOUR DATA: 'd' (daily stock data)\n",
    "    \n",
    "    dropout = 0.1         # Dropout rate (0.0 to 1.0)\n",
    "                          # Prevents overfitting\n",
    "                          # Typical: 0.1-0.3\n",
    "    \n",
    "    # ============================================\n",
    "    # OPTIONAL PARAMETERS\n",
    "    # ============================================\n",
    "    use_norm = 1          # Whether to use normalization (1=True, 0=False)\n",
    "    \n",
    "    freeze_encoder = False  # If True, freezes encoder (enc_embedding, TimesBlock layers, layer_norm)\n",
    "                            # and only trains the classifier head (projection layer)\n",
    "                            # Useful for transfer learning or fine-tuning scenarios\n",
    "                            # Default: False (train entire model)\n",
    "    \n",
    "    # ============================================\n",
    "    # EXAMPLE CONFIG FOR YOUR STOCK PREDICTION TASK\n",
    "    # ============================================\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_stock_config():\n",
    "        \"\"\"Example configuration for stock price classification\"\"\"\n",
    "        from types import SimpleNamespace\n",
    "        \n",
    "        config = SimpleNamespace()\n",
    "        \n",
    "        # Task\n",
    "        config.task_name = 'classification'\n",
    "        \n",
    "        # Data dimensions (from your data)\n",
    "        config.seq_len = 240      # 240-day lookback window\n",
    "        config.enc_in = 13       # 13 features per timestep\n",
    "        config.num_class = 2     # Binary classification (up/down)\n",
    "        \n",
    "        # Model architecture\n",
    "        config.d_model = 256     # Moderate size for efficiency\n",
    "        config.d_ff = 1024       # 4x d_model\n",
    "        config.e_layers = 2      # 2 TimesBlock layers\n",
    "        \n",
    "        # TimesBlock specific\n",
    "        config.top_k = 5         # Find top 5 periodic patterns\n",
    "        config.num_kernels = 6   # 6 convolution kernels\n",
    "        \n",
    "        # Embedding\n",
    "        config.embed = 'timeF'   # Time feature embedding\n",
    "        config.freq = 'd'        # Daily frequency\n",
    "        config.dropout = 0.1     # 10% dropout\n",
    "        \n",
    "        # Encoder freezing (optional)\n",
    "        config.freeze_encoder = False  # Set to True to freeze encoder and train only classifier head\n",
    "        \n",
    "        # Not used for classification but required\n",
    "        config.pred_len = 0\n",
    "        config.label_len = 0\n",
    "        config.c_out = config.enc_in\n",
    "        \n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc12fd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/loganyamamoto/Desktop/class/CSCI/566/project/StockPredictor/logan-version/nlp_features.py:60: UserWarning: transformers library not available. Install with: pip install transformers torch\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TrainerConfig created successfully!\n",
      "   Model type: TimesNet\n",
      "   Period type: LS\n",
      "   Early stop patience: 7\n",
      "   Sequence length (seq_len): 240\n",
      "   TimesNet enc_in: 13\n",
      "   TimesNet num_class: 3\n",
      "   TimesNet freeze_encoder: False\n",
      "[distributed] is_dist=False, rank=0/1\n",
      "[device] using mps\n",
      "[mps] Apple Metal Performance Shaders backend\n",
      "[dataloader] num_workers=1, persistent_workers=False, pin_memory=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading training dataset (.npz): 100%|██████████| 3/3 [00:00<00:00, 743.10it/s]\n",
      "Loading validation dataset (.npz): 100%|██████████| 3/3 [00:00<00:00, 1515.10it/s]\n",
      "Loading test dataset (.npz): 100%|██████████| 3/3 [00:00<00:00, 1936.73it/s]\n",
      "Loading metrics dataset (.npz): 100%|██████████| 3/3 [00:00<00:00, 3750.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[data] Loaded from cache\n",
      "[data] Data loaded successfully\n",
      "  X_train shape: torch.Size([9779, 31, 3])\n",
      "  X_val shape: torch.Size([2445, 31, 3])\n",
      "  X_test shape: torch.Size([3057, 31, 3])\n",
      "  Expected X shape: (num_samples, <seq_len=240, num_features) - sampled timesteps only\n",
      "  Actual sequence length: 31\n",
      "  Period type: LS (sampled timesteps, no gap filling)\n",
      "[WARNING] Data sequence length (31) differs from config seq_len (240). Expected: <240 (sampled) for period_type='LS'. Using actual data shape.\n",
      "[config] Data shape validation:\n",
      "  X_train shape: (9779, 31, 3)\n",
      "  Expected shape: (num_samples, 240, 3)\n",
      "  ✓ Feature dimension matches: 3 == 3\n",
      "  ⚠️ Sequence length: 31 != 240\n",
      "[config] Determined input_shape from data: (31, 3) (seq_len=31, features=3)\n",
      "[config] ⚠️  Warning: TrainerConfig.seq_len (240) differs from data sequence length (31). Using TrainerConfig value.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TimesNet] Model initialized with config:\n",
      "  seq_len: 240\n",
      "  enc_in: 13\n",
      "  num_class: 3\n",
      "  d_model: 200\n",
      "  freeze_encoder: False\n",
      "52957803 total parameters\n",
      "✅ Trainer initialized successfully!\n",
      "   Trainer device: mps\n",
      "   Trainer is_main: True\n",
      "   Model total parameters: 52,957,803\n"
     ]
    }
   ],
   "source": [
    "# Add logan-version to Python path to import trainer\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Get project root\n",
    "cwd = os.getcwd()\n",
    "if 'other_models' in cwd:\n",
    "    project_root = os.path.dirname(cwd)\n",
    "else:\n",
    "    project_root = cwd\n",
    "\n",
    "logan_version_path = os.path.join(project_root, 'logan-version')\n",
    "if logan_version_path not in sys.path:\n",
    "    sys.path.insert(0, logan_version_path)\n",
    "\n",
    "from trainer import TrainerConfig, Trainer\n",
    "from types import SimpleNamespace\n",
    "\n",
    "# Create TimesNet-specific model configuration\n",
    "# Note: seq_len is NOT included here - it should be set in TrainerConfig\n",
    "timesnet_config = SimpleNamespace(\n",
    "    # Task configuration\n",
    "    task_name='classification',\n",
    "    \n",
    "    # Data dimensions (adjust based on your actual data)\n",
    "    # seq_len is set in TrainerConfig, not here\n",
    "    enc_in=13,           # Number of features per timestep\n",
    "    num_class=3,         # Number of classification classes (binary: up/down)\n",
    "    \n",
    "    # Model architecture\n",
    "    d_model=200,         # Dimension of model embeddings (hidden dimension)\n",
    "    d_ff=800,           # Dimension of feed-forward network (usually 4x d_model)\n",
    "    e_layers=1,          # Number of encoder layers (TimesBlock layers)\n",
    "    \n",
    "    # TimesBlock specific parameters\n",
    "    top_k=3,             # Number of top frequencies to consider in FFT\n",
    "    num_kernels=5,       # Number of kernels in Inception_Block_V1\n",
    "    \n",
    "    # Embedding configuration\n",
    "    embed='timeF',       # Time features encoding type ('timeF', 'fixed', 'learned')\n",
    "    freq='d',            # Frequency for time features ('d' for daily)\n",
    "    dropout=0.1,         # Dropout rate\n",
    "    \n",
    "    # Not used for classification but required\n",
    "    pred_len=0,\n",
    "    label_len=0,\n",
    "    c_out=13,           # Usually set to enc_in\n",
    "    \n",
    "    # Encoder freezing (optional)\n",
    "    freeze_encoder=False  # If True, freezes encoder (enc_embedding, TimesBlock layers, layer_norm)\n",
    "                          # and only trains the classifier head (projection layer)\n",
    "                          # Useful for transfer learning or fine-tuning scenarios\n",
    ")\n",
    "\n",
    "# Create TrainerConfig with all parameters\n",
    "config = TrainerConfig(\n",
    "    stocks=[\"AAPL\", \"MSFT\", \"GOOGL\"],  # List of stock tickers\n",
    "    time_args=[\"1990-01-01\", \"2015-12-31\"],  # Time range arguments\n",
    "    batch_size=32,                      # Batch size for training\n",
    "    num_epochs=1000,                    # Number of training epochs\n",
    "    saved_model=None,                   # Path to saved model (None for new training)\n",
    "    prediction_type=\"classification\",    # Type of prediction task\n",
    "    k=10,                               # Number of top/bottom positions for portfolio\n",
    "    cost_bps_per_side=5.0,              # Transaction costs per side in basis points\n",
    "    save_every_epochs=25,               # Save model every N epochs (0 to disable)\n",
    "    model_type=\"TimesNet\",              # Model type\n",
    "    model_config=timesnet_config,       # TimesNet-specific configuration\n",
    "    early_stop_patience=7,              # Early stopping patience (epochs)\n",
    "    early_stop_min_delta=0.001,         # Minimum improvement for early stopping\n",
    "    period_type=\"LS\",                 # Period type for feature extraction (\"240L20S21\" or \"full\")\n",
    "    seq_len=240,                        # Sequence length for data generation and model architecture\n",
    "    use_nlp=False,                       # Whether to use NLP features\n",
    "    nlp_method=\"aggregated\"              # NLP method (\"aggregated\" or \"individual\")\n",
    ")\n",
    "\n",
    "print(\"✅ TrainerConfig created successfully!\")\n",
    "print(f\"   Model type: {config.model_type}\")\n",
    "print(f\"   Period type: {config.period_type}\")\n",
    "print(f\"   Early stop patience: {config.early_stop_patience}\")\n",
    "print(f\"   Sequence length (seq_len): {config.seq_len}\")  # Now from TrainerConfig\n",
    "print(f\"   TimesNet enc_in: {config.model_config.enc_in}\")\n",
    "print(f\"   TimesNet num_class: {config.model_config.num_class}\")\n",
    "print(f\"   TimesNet freeze_encoder: {config.model_config.freeze_encoder}\")\n",
    "\n",
    "# Create Trainer instance with the config\n",
    "trainer = Trainer(config=config)\n",
    "print(\"✅ Trainer initialized successfully!\")\n",
    "print(f\"   Trainer device: {trainer.device}\")\n",
    "print(f\"   Trainer is_main: {trainer.is_main}\")\n",
    "if hasattr(trainer, 'Model') and trainer.Model is not None:\n",
    "    total_params = sum(param.numel() for param in trainer.Model.parameters())\n",
    "    print(f\"   Model total parameters: {total_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c05846d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n",
      "--------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train 2/1000:   0%|          | 0/306 [00:00<?, ?it/s]/Users/loganyamamoto/Desktop/class/CSCI/566/project/StockPredictor/logan-version/nlp_features.py:60: UserWarning: transformers library not available. Install with: pip install transformers torch\n",
      "  warnings.warn(\n",
      "train 2/1000:   0%|          | 0/306 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Model.forward() missing 3 required positional arguments: 'x_mark_enc', 'x_dec', and 'x_mark_dec'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/class/CSCI/566/project/StockPredictor/logan-version/trainer.py:1233\u001b[39m, in \u001b[36mTrainer.train_one_epoch\u001b[39m\u001b[34m(self, epoch)\u001b[39m\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     Y_pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1234\u001b[39m     \u001b[38;5;66;03m# Check for NaN/Inf in model output\u001b[39;00m\n\u001b[32m   1235\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch.isnan(Y_pred).any() \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(Y_pred).any():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/class/CSCI/566/project/StockPredictor/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/class/CSCI/566/project/StockPredictor/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: Model.forward() missing 3 required positional arguments: 'x_mark_enc', 'x_dec', and 'x_mark_dec'"
     ]
    }
   ],
   "source": [
    "trainer.train_one_epoch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5fce4b",
   "metadata": {},
   "source": [
    "take in full time window"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
